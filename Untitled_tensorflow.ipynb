{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled_tensorflow.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1tG1CQstbVbZpH_wcn_op_7-JtVh9w4sF","authorship_tag":"ABX9TyNQxfLzeWOdn8cu43Aw3aqI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["google colab 에서 로컬 gpu 사용하기\n","\n","https://research.google.com/colaboratory/local-runtimes.html\n","\n","\\\n","\n","dataset 다운로드 주소. google drive에 넣어 사용하세요.\n","\n","https://www.kaggle.com/datasets/gauravduttakiit/bitcoin-tweets-16m-tweets-with-sentiment-tagged"],"metadata":{"id":"YEZ4GhddvnyW"}},{"cell_type":"code","source":["!pip install tensorflow-text\n","!pip install tf-models-official"],"metadata":{"id":"YjdVzho4_gMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3q4HAu2e7-f5","executionInfo":{"status":"ok","timestamp":1653492056585,"user_tz":-540,"elapsed":9352,"user":{"displayName":"홍길동","userId":"07131602995978252295"}}},"outputs":[],"source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub # download preprocess and BERT-model from hub\n","import tensorflow_text as text\n","from official.nlp import optimization # adamW\n","\n","import matplotlib.pyplot as plt\n","\n","tf.get_logger().setLevel('ERROR')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/') # use mbsa.csv in gdrive"],"metadata":{"id":"LcMCI1SGf65x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mbsa.csv를 처리하여 mbsa_processed.csv를 생성\n","path = '/content/gdrive/My Drive/Colab Notebooks' # dataset path\n","data = pd.read_csv(path+\"/mbsa.csv\")\n","columns_to_keep = [\"Sentiment\"] + [\"text\"] # only use \"sentiment\" and \"text\"\n","data = data[columns_to_keep]\n","data = data[:10000000] # 학습시킬 데이터 양 (전체 19,344,048개)\n","#data.to_csv(\"mbsa_processed.csv\", index=False)\n","\n","#data = pd.read_csv(\"mbsa_processed.csv\")\n","\n","print(data['text'])"],"metadata":{"id":"S-jjguH-RLwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sentiment(x):\n","  if x == 'Positive':\n","    return 1\n","  elif x == 'Negative':\n","    return 2\n","  else:\n","    return 0\n","\n","# remove tags from tweets\n","def process_text(sentence):\n","  #tags = r\"@\\S*|#\\S+|http\\S*\" # remove @tags #tags httpaddress\n","  tags = r'http\\S*' # remove httpaddress\n","  sentence = str(sentence)\n","  sentence = re.sub(tags,\"\",sentence)\n","  return sentence\n","\n","data.text = data.text.map(process_text)\n","data.Sentiment = data.Sentiment.map(sentiment).astype(np.int32)\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data['text'], data['Sentiment'], test_size=0.2, random_state=1)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","\n","def make_dataset(Text, Label, batch_size, seed):\n","    # code from\n","    # text_dataset_from_directory\n","    # from_tensor_slices\n","    labels = 'inferred'\n","    label_mode = 'int'\n","    \n","    # to_categorical\n","    # [0] -> [1, 0, 0]\n","    # [1] -> [0, 1, 0]\n","    # [2] -> [0, 0, 1]\n","    # if you use loss=CategoricalCrossentropy, you must categorize label first.\n","    label_ds = tf.keras.utils.to_categorical(Label)\n","    label_ds = tf.data.Dataset.from_tensor_slices(label_ds)\n","    \n","    string_ds = tf.data.Dataset.from_tensor_slices(Text)\n","\n","    ds = tf.data.Dataset.zip((string_ds, label_ds))\n","    \n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","    ds = ds.shuffle(buffer_size=batch_size * 8, seed=seed)\n","    ds = ds.batch(batch_size)\n","    return ds\n"],"metadata":{"id":"JcgNr87pDTwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32 # 메모리가 적을 경우 2, 4, 8, 16\n","seed = 42\n","\n","train_ds = make_dataset(X_train, y_train, batch_size, seed)\n","val_ds = make_dataset(X_val, y_val, batch_size, seed)\n","test_ds = make_dataset(X_test, y_test, batch_size, seed)"],"metadata":{"id":"Bi4PuHNVM0pT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_train_ds = train_ds\n","my_val_ds = val_ds\n","my_test_ds = test_ds"],"metadata":{"id":"FXitGUWJl-Ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 3 # neutral, positive, negative"],"metadata":{"id":"BrNKaBuKN6xR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_train_ds = my_train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","my_val_ds = my_val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","my_test_ds = my_test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"],"metadata":{"id":"EkJjdPkPl-uX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model_name = 'bert_multi_cased_L-12_H-768_A-12'\n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name] # download BERT-model from hub\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name] # download preprocess from hub (=tokenizer)\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"metadata":{"id":"LIiWiB71_9e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example\n","\n","bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","text_test = ['this is such an amazing movie!']\n","text_preprocessed = bert_preprocess_model(text_test)\n","\n","#print(f'Keys       : {list(text_preprocessed.keys())}')\n","#print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n","#print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"]}')\n","#print(f'Input Mask : {text_preprocessed[\"input_mask\"]}')\n","#print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')\n","\n","bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","bert_results = bert_model(text_preprocessed)\n","\n","#print(f'Loaded BERT: {tfhub_handle_encoder}')\n","#print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n","#print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"]}')\n","#print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n","#print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"]}')"],"metadata":{"id":"iMjjQMBDAC2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_classifier_model():\n","    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","    encoder_inputs = preprocessing_layer(text_input)\n","    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","    outputs = encoder(encoder_inputs)\n","    net = outputs['pooled_output']  # pooled_output == CLS vector. normally used\n","                                    # mean pooling = mean of sequence_output, max pooling = convolution of sequence_output\n","    # neural network layer\n","    net = tf.keras.layers.Dropout(0.1)(net) # prevent overfitting\n","    net = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(net) # setting use_bias, kernel_initializer, kernel_regularizer, bias_regularizer, etc\n","    return tf.keras.Model(text_input, net)"],"metadata":{"id":"Ue7bgDXCAG9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_model = build_classifier_model()\n","\n","# example\n","#bert_raw_result = classifier_model(tf.constant(text_test))\n","#print(tf.constant(text_test))"],"metadata":{"id":"-5ndnnNIAH7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import backend as K\n","\n","def balanced_recall(y_true, y_pred):\n","    \"\"\"This function calculates the balanced recall metric\n","    recall = TP / (TP + FN)\n","    \"\"\"\n","    recall_by_class = 0\n","    # iterate over each predicted class to get class-specific metric\n","    for i in range(y_pred.shape[1]):\n","        y_pred_class = y_pred[:, i]\n","        y_true_class = y_true[:, i]\n","        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        recall_by_class = recall_by_class + recall\n","    return recall_by_class / y_pred.shape[1]\n","\n","def balanced_precision(y_true, y_pred):\n","    \"\"\"This function calculates the balanced precision metric\n","    precision = TP / (TP + FP)\n","    \"\"\"\n","    precision_by_class = 0\n","    # iterate over each predicted class to get class-specific metric\n","    for i in range(y_pred.shape[1]):\n","        y_pred_class = y_pred[:, i]\n","        y_true_class = y_true[:, i]\n","        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        precision_by_class = precision_by_class + precision\n","    # return average balanced metric for each class\n","    return precision_by_class / y_pred.shape[1]\n","\n","def balanced_f1_score(y_true, y_pred):\n","    \"\"\"This function calculates the F1 score metric\"\"\"\n","    precision = balanced_precision(y_true, y_pred)\n","    recall = balanced_recall(y_true, y_pred)\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"],"metadata":{"id":"tWAmCt4WAI1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 5\n","\n","\n","METRICS = [\n","    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n","    balanced_recall,\n","    balanced_precision,\n","    balanced_f1_score\n","]\n","\n","# prevent overfitting with (X_val, y_val)\n","earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, restore_best_weights = True)\n","\n","# optimizer adamw\n","steps_per_epoch = tf.data.experimental.cardinality(my_train_ds).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","init_lr = 3e-5\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')"],"metadata":{"id":"M3xqg7RQAJxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_model.compile(optimizer=optimizer, # classifier_model + optimizer + loss function + metrics\n","                         loss='categorical_crossentropy',\n","                         metrics=METRICS)"],"metadata":{"id":"XUgPKG0pALTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Training model with {tfhub_handle_encoder}')\n","history = classifier_model.fit(my_train_ds,\n","                               validation_data=my_val_ds,\n","                               epochs=epochs,\n","                               callbacks=[earlystop_callback])"],"metadata":{"id":"kCEOzy1_AMWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# dataset -> list\n","lx_test = [d.numpy() for d in my_test_ds.unbatch().map(lambda x,y:x)]\n","# dataset -> list -> argmax\n","ly_test = np.argmax([d.numpy() for d in my_test_ds.unbatch().map(lambda x,y:y)], axis=1)\n","y_pred = [np.argmax(pred) for pred in classifier_model.predict(lx_test)]\n","print(classification_report(ly_test, y_pred))"],"metadata":{"id":"2oFzC9qWAOLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_model.save(\"/content/gdrive/My Drive/Colab Notebooks/natural/models/natural_v1\")\n","classifier_model = tf.keras.models.load_model(\"/content/gdrive/My Drive/Colab Notebooks/natural/models/natural_v1\", custom_objects={'balanced_recall':balanced_recall, 'balanced_precision':balanced_precision, 'balanced_f1_score':balanced_f1_score, 'AdamWeightDecay': optimizer})"],"metadata":{"id":"N1hjCVNlAO_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_model.summary()"],"metadata":{"id":"w32_8xvgSApZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["in_sentences = [\"never again\",\n","                \"very very good\"]\n","result = classifier_model(tf.constant(in_sentences))\n","my_labels = [\"neutral\", \"positive\", \"negative\"]\n","\n","for re in result:\n","  print(f'{my_labels[np.argmax(re)]} {np.max(re)}')"],"metadata":{"id":"rtBBI-XDs_TW"},"execution_count":null,"outputs":[]}]}